% File example.tex
% Contact: simonnet@ecole.ensicaen.fr
%
% version 1.0 (July 24, 2009)
% version 1.1 (September 3, 2009)
% add using of the optional command: \secondauthoraddr

\documentclass[10pt]{article}

\input{includes/icdp2009template.tex}

%other package

% vectorial font
%\usepackage{lmodern}

\usepackage{graphicx}
\usepackage{times}

\usepackage{textcomp}
\usepackage[justification=centering]{caption}
\usepackage{subcaption}
\usepackage{enumitem}
\usepackage[caption=false]{subfig}
\usepackage{enumitem}
\usepackage{multirow}

\begin{document}
\noindent

% This should produce references in the order they appear
\bibliographystyle{ieeetr}

%\title{Automatically landmarks prediction on Beetle's pronotum}
%\title{Deep network for landmarks prediction on Beetle pronotums}
%\title{Two scenarios to predict landmarks on Beetle pronotum by Deep Network}
\title{Towards landmarks prediction with Deep Network}

\authorname{Van Linh Le$^{1,3}$, Marie Beurton-Aimar$^{1}$, Akka Zemmari$^1$, Nicolas Parisey$^2$}
\authoraddr{$^1$LaBRI - CNRS 5800 Bordeaux University, France, van-linh.le/beurton/zemmari@labri.fr}

%optional
\secondauthoraddr{$^2$IGEPP - INRA 1349, France, nparisey@rennes.inra.fr }
\thirdauthoraddr{$^3$ITDLU - Dalat University, Vietnam, linhlv@dlu.edu.vn}


\maketitle

\keywords
Landmarks, convolutional neural networks, fine-tuning, recogntion, procrustes.

\abstract
Morphometry landmarks are used in many biological
applications. Mostly, the landmarks are defined manually or
semi-automatic by applying the image processing techniques. In recent
years, deep learning is known as a good solution for the difficult
problems in computer vision. It appears in many fields such as
classification, recognition, face detection. In the context applying
deep learning to solve the regression problems, in this paper, we
present a convolutional neural network to predict the landmarks on 2D
images, specify beetle's images. The experiments on the
proposed network have been done in two ways: training from scratch and
fine-tuning from a trained model. The dataset includes the images of 
collecting from $253$ beetles. For each beetle, the images of 5 parts 
have been extracted(\textit{head, pronotum, body, left and right mandible}).
 Among these, a set of manual landmarks has been built for each part by 
the biologists. In this task, we use the images of 3 parts (head, pronotum, and body) with the main purpose to predict the landmarks on pronotum images.
 The quality of predicted landmarks
is evaluated by calculating the distance in pixels between the
coordinates of the predicted landmarks and manual landmarks which are
considered as ground truth.


\section{Introduction}
Morphometrics landmarks (or point of interest) is important features
in many biological investigations. They are  usually used to analyze
the forms of biological organs or organisms. Their analysis is mainly
based on their coordinates. Depending on the problem to study, the number of
landmarks may be more or less high and  their location  can
be on the shape (border) or inside the object. For
  examples, the landmarks on Drosophila wings \cite{drosophilaWings} have stayed on the
veins of the wings but the landmarks on human ear \cite{cintas2016automatic} can be located at
the ear border or inside it. Currently, the landmarks are set manually by
the biologists, one can note that this work is time-consuming and difficult to
reproduce, therefore, a method that proposes automatically the
coordinates of landmarks could be a concern.

In image processing, segmentation is most often the first and the most
important step. This task remains a bootleneck to compute features of
an image. In some cases, the object of interest is easy to extract and
can be analyze with the help of a lot of very well-known image
analysis procedures. In a previous studying \cite{le2017maelab}, we have analyzed beetle
mandibules with a set of algorithms based on the Hough Transform
procedure \cite{palaniswamy2010automatic}, SIFT
\cite{lowe2004distinctive} and SURF \cite{bay2006surf} algorithms could also be
suitable to work on this topic for example. But in some cases, the question of how to segment the
object of interest consumes the most part of a time project. It is why
we have turned our studying to a way to analyze image without a
segmentation step. The application has been again on beetles images
but on \textit{pronotum, head, body} parts. As these parts have not
been separated from each other, their segmentation by image processing
procedures has been given up. Coordinates of manual landmarks for each part
have been provided and are considered as the ground
truth to evaluated the predicted landmarks by the algorithms. We have focus on the
pronotum parts for this studying, see (Fig.\ref{figpronotum}) to see
the $8$ landmarks that we are looking for.


To achieve the landmarks prediction, a Convolutional Neural Network
(CNN)\cite{lecun2010convolutional} has been designed based on Lasagne
library\cite{lasagne}. From a first model version, the network has been
trained from scratch on the dataset of pronotum images. In a second
step the training has been modified to include a fine-tuning
\cite{yosinski2014transferable} stage.



\begin{figure}[htbp]
\centering
	\centerline{\includegraphics[scale=0.7]{images/pronotum}}
	\caption{\small{An example of pronotum images and its manual landmarks}}
	\label{figpronotum}
\end{figure}

In the next section, we present related works about automatic
estimation landmarks on 2D images. In section $3$, we present the
architecture of the network and the procedure to enlarge the data
set. In section $4$ we compare the results obtained with the first
model and these ones after fine-tuning. 


\section{Related works}
Deep learning methods are coming from machine learning theory. They
have been introduced in the middle of previous century for artificial
intelligence applications but their application encounters problems to
take into account real cases. More recently, the improvement of computing capacities, both in
memory size and time with GPU programming has opened a new challenge  
for deep learning. Many deep learning architectures have been proposed
to solve the problems of classification \cite{krizhevsky2012imagenet,
  ciregan2012multi}, image recognition \cite{szegedy2015going,
  farabet2013learning, li2015convolutional}, speech recognition
\cite{mikolov2011strategies, hinton2012deep} and language translation
\cite{jean2014using, sutskever2014sequence}. Along with that
developments, many frameworks have been built such as Caffe
\cite{jia2014caffe}, Theano \cite{2016arXiv160502688short}, Tensorflow
\cite{tensorflow2015},.... These frameworks help
the users to design their application by re-using networks
architecture they propose. In image analysis domain,
deep learning, specifically with CNN, can be used to predict the key points on
the image. Yi Sun et al. \cite{sun2013deep} have proposed a cascaded
convolutional network to predict the key points on the human
face. Zhang et al. \cite{zhang2014facial} optimizes facial landmarks
detection with a set of related tasks such as head pose estimation,
age estimation, .... Cintas et al. \cite{cintas2016automatic} have
introduced a network to predict the landmarks on human ear images to
characterize ear shape.


In geometric morphometry, landmarks or points of interest are one of
the important features. Landmark studies have traditionally
analyzed on 2D images. Depending on if the analyzed images are
easy or not to segment, setting landmarks can apply
the different methods. When segmentation can be applied, Lowe et
al. \cite{lowe2004distinctive} have proposed a method to identify the
key points in the 2D image. From the detected key points, the method
is able to match two images. Palaniswamy et
al. \cite{palaniswamy2010automatic} have applied probabilistic Hough
Transform to automatically estimate the landmarks in images of
Drosophila wings. Krahenbuhl et al. \cite{le2017maelab} have extended
Palaniswamy's method to detect landmarks automatically on beetles
mandibles. Unfortunately, after testing, when the segmentation has not good quality,
we have observed that re-using this method produces too many
noises. This is why we have turned our work on deep learning
algorithms in order to find suitable solution to predict the landmarks
on images hard to segment.


\section{Network model}
Deep learning is a learning method with multiple levels of
representation of connected layers (convolutional neural
network). Data representation is transformed from a lower level to a
higher one with many complex functions that can be learned via
backpropagation. In this section, we present the initial version of the CNN that we have used
to begin to predict the landmarks. 

\subsection{Network architecture}
\label{secmodel}
The first step to work in CNN is to study the network
architecture. After several tests, we have chosen for this application
to work with a model provided in Lasagne framework \cite{lasagne} coming from
Theano \cite{2016arXiv160502688short}. In the first section, we will present the original model
and then, we will describe how we have modified it by definition of an
elementary block that we compose in the final model.


Like the networks have been proposed by LeCun et al. \cite{lecun2010convolutional}, Li et al. \cite{li2015convolutional}, and Cintas et al. \cite{cintas2016automatic}, the proposed network consists of common layers
with different learnable parameters. It receives an input of $1 \times
256 \times 192$ to train, to validate, and to  test. The network
consists of three repeated-structures of a convolutional layer
followed by a maximum pooling layer. The depth of convolutional layers increases with different size of the filter kernels.
All the kernels of pooling layers have the same size. 
At the end, three full connected layers have been added to the
network. The output of the last full-connected
layer corresponds to the $16$ values which are the coordinates of the
$8$ landmarks to predict.




The experiment on original model shows that this architecture is still
not good enough to predict the landmarks. It is still general and
overfitting has appeared during training and validation.
To prevent the overfitting, four dropout layers have been added into
the network. These are considered as the good solution to prevent the
overfitting. The idea of dropout is to randomly drop units from the
neural network during training
\cite{srivastava2014dropout}. Fig.\ref{cnnnetwork2} presents the final
architecture of the model. The first three dropout layers are
supplemented to the repeated-structures followed the maximum pooling
layers. In that way, structure becomes an elementary block includes a
convolution layer (\textit{$C_i$}) followed by a maximum pooling
(\textit{$P_i$}) and dropout layer (\textit{$D_i$})(with
$i=1..3$). The probability values used for dropout layers are $0.1$,
$0.2$, and $0.3$. Actually, we keep the same value for the parameters
of the convolutional, pooling and full-connected layers as the
previous architecture.

The remaining dropout layer (\textit{$D_4$}) is inserted between the first
two full connected layers (\textit{$FC_1$ and $FC_2$}). The probability value of this layer
is set to $0.5$. The output layer (\textit{$FC_3$}) still contains 16
units corresponding to the coordinates of 8 predicted landmarks.
For each elementary blocks, the layers and parameters are as below, the order of parameter values of each layer corresponding to the order of each in elementary blocks.

\begin{itemize}[nosep,label=\footnotesize$\bullet$]

\item CONV layers:
		\begin{itemize}[nosep]
			\item Number of filters: $32, 64,$ and $128$,
			\item Kernel filters size: $(3 \times 3), (2 \times 2),$ and $(2 \times 2)$,
			\item Stride values: $1, 1, 1$,
			\item No padding is used for CONV layers.
		\end{itemize}			
	\item POOL layers:
		\begin{itemize}[nosep]
			\item Kernel filters size: $(2 \times 2), (2 \times 2),$ and $(2 \times 2)$,
			\item Stride values: $2, 2, 2$.
			\item No padding is used for POOL layers.
		\end{itemize}
	\item DROP layers: 
		\begin{itemize}[nosep]
			\item Propabilities: $0.1, 0.2, $ and $0.3$.
		\end{itemize}
	\end{itemize}
The last fool connected layers (FC) parameters are : FC1 output :
$1000$, FC2 output : $1000$, FC3  output : $16$. A dropout layer is
inserted between FC1 ond FC2 with probability equal to $0.5$.


\begin{figure*}[!t]
\centering
\includegraphics[scale=0.32]{images/arch_model}
\caption{{\small{Network architecture using $3$ elementary block.
  Convolution
  layer in red, pooling in yellow and dropout in green color.}}} 
\label{cnnnetwork2}
\end{figure*}

During training, the values of learnable parameters have been updated to increase the accuracy of the network by applying gradient descent in backward phase. Therefore, the network is designed with a small sharing learning rate and momentum. Their values are updated over training time to fit with the number of epochs\footnote{An epoch is a single pass through the full training set.}. The network is designed to finish the training in $5000$ epochs. The learning rate was initialized at $0.03$ and stopped at $0.00001$, while the momentum was updated from $0.9$ to $0.9999$. 
Because landmarks prediction can be seen as a regression problem in deep learning. Therefore, the root mean square error (RMSE) was used as a quality metric to evaluate the result and compute the losses of the proposed architecture. The implementation of the network is done on Lasagne framework \cite{lasagne} which allows computing on GPU. The network has been trained on NVIDIA TITAN X cards.

\subsection{Enlarge the dataset}
\label{sec_data}
The dataset includes $293$ images of beetles (for each part). All images are taken with the same camera in the same condition with a resolution of $3264 \times 2448$. Each image has been set $8$ manual landmarks by biologists (Fig. \ref{figpronotum}). The dataset was split into two subsets: training (and validation) set contains $260$ images and testing set includes $33$ images. In most of CNNs \cite{lecun2010convolutional, sun2013deep,  krizhevsky2012imagenet, cintas2016automatic}, the size of the input is limited to $256$ pixels. In our case, the resolution of input image seems that too large and it becomes a difficulty for the network. So, the images are down-sampling to a new resolution $256 \times 192$ before training and testing. Of course, the coordinates of manual landmarks are also scaled to fit with the new resolution of images.

Besides the size of the input, the number of images is also a challenge when applying CNN. Normally, training a CNN with a large dataset will give us the result better than when we training CNN on a small dataset. Moreover, working with a small dataset, we can meet a popular problem, \textit{overfitting}. So, we need to enlarge the size of the dataset instead of $260$. In image processing, we usually apply transform procedures (translation, rotation) to generate a new image but in fact, when we compute the value of the pixels, it does not change while CNN computes the values of the pixels. To address this problem, we have applied two procedures to enlarge the size of the dataset.

The first procedure is applied to change the value of each
channel in the original image. According to that, a constant is
added to a channel of RGB image and for each time, we just
change the value of one of three channels. For example, from
an original RGB image, if we add a constant c = 10 to the
red channel, we will obtain a new image with the values at
red channel by greater than the red channel of original image
a value of 10. By this way, we can generate three new RGB
images from a RGB image.

The second procedure is splitting the channels of RGB
images. It means that we separate the channels of RGB into
three gray-scale images. This work seems promising because
the network works on single-channel images. At the end of the procedures, we
can generate six versions from an image, the total number of
images used to train and validate is $260 \times 7 = 1820$ images
(six versions and original image).


\subsection{First results}
\label{sectrain1}
In this scenario, the network was trained on a dataset of $1820$ pronotum images which were generated from $293$ original pronotum images by applying the procedure in section \ref{sec_data}. The number of images that
used for training and validation is splitted randomly by a ratio
(training: $60\%$, validation: $40\%$) that has been set during the
network setup. During the training, the network learned the information through a pair of \textit{(image, landmarks)} in training set. At the test phase, the image without landmarks was given to the trained network and the predicted landmarks will be given at the output. In practical of CNN, convergence is
usually faster if the average of each input variable over the
training set is close to zero. Moreover, when the input is set
closed with zero, it will be more suitable with the sigmoid
activation function \cite{lecun2012efficient}. According to \cite{lecun2012efficient}, the brightness of
the image is normalized to $[0; 1]$, instead of $[0; 255]$ and the
coordinates of the landmarks are normalized to $[-1; 1]$, instead
of $[0; 256]$ and $[0; 192]$ before giving to the network.



To obtain all the predicted landmarks for all pronotum images (instead of $33$ images), a situation to choose the test images is executed, called \textit{round}. For each round, a set of 33 images have been chosen for the test set; the remaining images have been put into the training set. Following that, the network will be trained with many different training datasets and the output model will be used to predict the landmarks on the images in the corresponding test set. Table.\ref{tbltrainingloss} shows the losses during training the network on pronotum images.

\begin{table}[h!]
	\centering
	\begin{tabular}{l l l}
	Round & Training loss & Validation loss \\ \hline
	1 & 0.00018 & 0.00019  \\ \hline
	2 & 0.00019 & 0.00021 \\ \hline
	3 & 0.00019 & 0.00026 \\ \hline
	4 & 0.00021 & 0.00029 \\ \hline
	5 & 0.00021 & 0.00029 \\ \hline
	6 & 0.00019 & 0.00018 \\ \hline
	7 & 0.00018 & 0.00018 \\ \hline
	8 & 0.00018 & 0.00021 \\ \hline
	9 & 0.00020 & 0.00027 \\ \hline
	\end{tabular}
	\caption{The losses during training the model on pronotum images dataset}
	\label{tbltrainingloss}
\end{table}

Besides the losses during training, the accuracy on coordinates of
predicted landmarks in the test images is also considered. Firstly,
the trained model was used to predict the landmarks on all images in
the test set. Then, the distances (in pixels) between manual and
corresponding predicted landmarks in each image were calculated as the
error distances. Finally, the error distance per landmark was
calculated for all test images. Table.\ref{tabledistance} shows the
average error distance given on each landmark. With the size of the
images is $256 \times 192$, if we accept an error around $3\%$ of the
image size ($ \sim3.5$ pixels), the error distances are acceptable.

\begin{table}[h!]
	\begin{subtable}{.22\linewidth}
	\centering
	\begin{tabular}{|c|c| }
	\hline
	\textbf{$\#$Landmark} & \textbf{Distance} \\ \hline
	1 & 4.002  \\ \hline
	2 & 4.4831 \\ \hline
	3 & 4.2959 \\ \hline
	4 & 4.3865 \\ \hline
	
	\end{tabular}
	\end{subtable}%
	\hspace{2.5cm}
	\begin{subtable}{.2\linewidth}
	\centering
	\begin{tabular}{|c|c| }
	\hline
	\textbf{$\#$Landmark} & \textbf{Distance} \\ \hline
	5 & 4.2925 \\ \hline
	6 & 5.3631 \\ \hline
	7 & 4.636 \\ \hline
	8 & 4.9363 \\ \hline
	\end{tabular}
	\end{subtable}
	\caption{The average error distance per landmark}
	\label{tabledistance}
\end{table}

Fig.\ref{figrsexample} shows the predicted landmarks on two test
images. When we consider the accuracy of predicted landmarks by
calculating the distance between manual and corresponding predicted
landmarks, the accuracy on coordinates of predicted landmarks on
Fig.\ref{figsub1} is $99\%$ and the propotion on Fig.\ref{figsub2} is
$80\%$.


\begin{figure}[htbp]
    \centering
    \begin{subfigure}[t]{0.25\textwidth}
        \centering
        \includegraphics[height=1.2in]{images/fn_accuracy}
        \caption{\small{Image with well-predicted landmarks}}
        \label{figsub1}
    \end{subfigure}%
    ~ 
    \begin{subfigure}[t]{0.25\textwidth}
        \centering
        \includegraphics[height=1.2in]{images/plandmark2}
        \caption{\small{Image with inaccuracy landmarks}}
        \label{figsub2}
    \end{subfigure}
    \caption{\small{The predicted landmarks on the images in test set.\\
    		 The red points present for the predicted landmarks}}
    \label{figrsexample}
\end{figure}


As a result, the network is able to predict the landmarks on a test
set of pronotum images. At statistic side, the predicted landmarks are
acceptable. But in image processing side, we expect more about the
accuracy (coordinates of predicted landmarks), and the result of CNN
is still needed to improve.

\section{Fine-tuning to transfer learning}
\label{secimproving}
In section \ref{sectrain1}, the proposed network has been experimented
by training from scratch on pronotum dataset. The results of
experiments have shown that the network has worked well to detect the
landmarks on the pronotum images. However, when we consider the
predicted landmarks by displaying the landmarks on the images, the
result is still not precise, the average error still high ($\geq 4$
pixels).


In order to reach more acceptable results for biologists, we have
broadened the model with the step of transfer learning. That is a
method that we re-use the model developed for a task on another
task. It allows rapid progress or improved the performance of the
model on the second task \cite{torrey2009transfer}. In our case, we
have applied fine-tuning, a strategy of transfer learning. Fine-tuning
is to not only replace and retrain the model on the new dataset, it
also fine-tunes the weight of trained model by continuing the
backpropagation. So, instead of training and testing on pronotum
images dataset, the network will be trained on the dataset includes
the images of three parts of beetle i.e pronotum, body and head. Then,
the trained model will be used to fine-tune and test on pronotum set.


\subsection{Training data preparation}
The training dataset includes a combination of the images from three
sets: pronotum, body and head (Fig.\ref{figshape3parts}). For each
set, $260$ original images have been chosen radomly for training and
validation. By applying the same procedure in section \ref{sec_data},
the training dataset was enlarged to $5460$ images ($260 \times 7
\times 3$). However, the number of manual landmarks on each part is
difference: \textit{$8$ landmarks on pronotum part, $11$ landmarks on
  body part, and $10$ landmarks on head part}. The manual landmarks
have a specific meaning for the biologists. So, we can not insert the
landmarks arbitrarily. Instead of, we will keep the smallest number of
landmarks among three parts and we remove some landmarks on other
parts. Therefore, we kept the number of the landmark on pronotum as a
reference and we suppressed some landmarks on the body and head
part. Specifically, we have removed three landmarks on the body part
($1^{st}; 6^{t}h; 9^{th}$) and two landmarks on the head part ($5{th};
6^{th}$) (Fig.\ref{figshape3parts}).


\begin{figure}[htbp]

        \centering
        \includegraphics[scale=0.25]{images/merge}
 
 
    \caption{\small{A presentation of head, pronotum and body part with corresponding manual landmarks}} 
    \label{figshape3parts}
\end{figure}

\subsection{Using fine-tuning for pronotum dataset}
The training data includes $5460$ images was trained on the proposed network (section \ref{secmodel}) with the same parameters than we have trained on pronotum images. After that, the trained model have been continued to fine-tune on pronotum dataset. To compare the result with the previous one, the trained model has been fine-tuned in many rounds with different datasets. The losses during fine-tuning are shown in Table.\ref{tblfinetuningloss}. Comparing with the losses when we trained the model from scratch (Table. \ref{tbltrainingloss}), the validation losses of this scenario are significantly improved (around $40\%$).
\begin{table}[h!]
	\centering
	\begin{tabular}{l l l}
	Round & Training loss & Validation loss \\ \hline
	1 & 0.00019 & 0.00009  \\ \hline
	2 & 0.00018 & 0.00010 \\ \hline
	3 & 0.00018 & 0.00010 \\ \hline
	4 & 0.00019 & 0.00008 \\ \hline
	5 & 0.00019 & 0.00009 \\ \hline
	6 & 0.00018 & 0.00008 \\ \hline
	7 & 0.00019 & 0.00008 \\ \hline
	8 & 0.00018 & 0.00006 \\ \hline
	9 & 0.00018 & 0.00009 \\ \hline
	\end{tabular}
	\caption{The losses during fine-tuning model}
	\label{tblfinetuningloss}
\end{table}

The output model has been used to predict the landmarks on the test images. Then, the average errors based on the distances between predicted and corresponding manual landmarks are given. The results are shown in Table.\ref{tab2}. The \textbf{Error 1} column presents for the average error during training from scratch on pronotum images; the \textbf{Error 2} column presents for the average error during fine-tuning the pronotum from the trained model; and the \textbf{Percentage} column presents for the difference between the distance errors in percentage. Clearly, when we compare the average errors between two scenarios, the result of predicted landmarks in the second scenario is more precise than the first one.


\begin{table}[htbp]
\centering
\begin{tabular}{ | c | c | c | c | c | }
\hline
	\multicolumn{1}{|c|}{\multirow{2}{*}{Landmark}} & \multicolumn{2}{c|}{CNN} &  \multicolumn{2}{c|}{Fine-tuning}  \\ \cline{2-5}
	 & Average & SD & Average & SD \  \\ \hline
	\textbf{LM1} & \textbf{4.002} & \textbf{2.5732} & \textbf{2.486} & \textbf{1.5448} \\ \hline
	LM2 & 4.4831 & 2.7583 & 2.7198 & 1.7822 \\ \hline
	LM3 & 4.2959 & 2.7067 & 2.6523 & 1.8386 \\ \hline
	LM4 & 4.3865 & 3.0563 & 2.7709 & 1.9483 \\ \hline
	LM5 & 4.2925 & 2.9086 & 2.4872 & 1.6235 \\ \hline
	\textbf{LM6} & \textbf{5.3631} & \textbf{3.4234} & \textbf{3.0492} & \textbf{1.991} \\ \hline
	LM7 & 4.636 & 2.8426 & 2.6836 & 1.7781 \\ \hline
	LM8 & 4.9363 & 3.0801 & 2.8709 & 1.9662 \\ \hline
\end{tabular}
\caption{A comparing between the average error distances, the standard deviation values per landmark of two steps.}
\label{tab2}
\end{table}

Fig.\ref{figrsexample2} shows the distribution of error distances on two landmarks. We can see that in both two cases, the result when we fine-tune the model is more improved than the first result.
\begin{figure}[htbp]
   
    \begin{subfigure}[t]{0.25\textwidth}
        \centering
        \includegraphics[scale=.35]{images/lm1_cnn_2}
        \caption{\small{Landmark 1 - CNN}}
        \label{figsub11}
    \end{subfigure}%
    ~ 
    \begin{subfigure}[t]{0.25\textwidth}
        \centering
        \includegraphics[scale=.34]{images/lm1_finetuning_2}
        \caption{\small{Landmark 1 - fine-tuning}}
        \label{figsub22}
    \end{subfigure}~\\
    \begin{subfigure}[t]{0.25\textwidth}
        \centering
        \includegraphics[scale=.33]{images/lm6_cnn_2}
        \caption{\small{Landmark 6 - CNN}}
        \label{figsub111}
    \end{subfigure}%
    ~ 
    \begin{subfigure}[t]{0.25\textwidth}
        \centering
        \includegraphics[scale=.32]{images/lm6_finetuning_2}
        \caption{\small{Landmark 6 - fine-tuning}}
        \label{figsub222}
    \end{subfigure}
    \caption{\small{The distribution of distance error on $1^{st}$ and $6^{th}$ landmarks of all images in two testing step(CNN and fine-tuning). The blue and red lines present for the average distances and standard deviation values, respectively.}}
    \label{figrsexample2}
\end{figure}

As a result of working, the program outputs the
predicted-landmarks of the images as TPS files. With the outputs are TPS files,
the user can use MAELab framework\footnote{MAELab is a free software written in C++. It can be directly and freely
obtained by request at the authors.} to display the
landmarks on the images.
\section{Conclusion}
In this paper, we have presented a CNN with two scenarios to predict the landmarks on beetle's pronotum images. The CNN network has been designed with three times repeated structure which consists of a convolutional layer, a max pooling layer, and a dropout layer, followed by the connected layers. In the training phase, the CNN have been trained with several times in different selections of training data. After training, the network was able to predict the landmarks on the images in the test set. 

In the first scenario, the model has been trained (from scratch) and tested on the dataset of pronotum images. While in the second scenario, the model has been trained on a dataset includes the images from three parts of beetles. Then, the trained model has been used to fine-tune and test on pronotum images.

The result has been evaluated by comparing the coordinates between predicted and manual landmarks.  The results have shown that using the convolutional network to predict the landmarks on biological images is promising good results in the case that the image was difficult to segment. The quality of prediction allows using automatic landmarking to replace manual landmarks in some aspects. Training model from scratch or fine-tuning the trained model are given the acceptable results. But with a limited number of data, we need to improve the results a little bit. Therefore, future research in landmarking identification appears as an improved of the worth exploring.

\section*{Acknowledgements}
The research has been supported by DevMap project. We would like to thank the my colleague, ALEXIA Marie, who have provided manual landmarks on beetle images.

\bibliography{IEEEabrv,includes/icdp2009}


\end{document}






















the predicted landmarks by applying the second scenario on the same test images as Fig.\ref{figrsexample}. The red points present for the predicted landmarks. In Fig.\ref{figsub11}, the positions of predicted landmarks are the same when we compare with the result from Fig.\ref{figsub1}; but in Fig.\ref{figsub22}, the coordinates of predicted landmarks are strongly improved.
\begin{table}[htbp]
\centering
\begin{tabular}{|c|p{1.5cm}|}
\hline
\textbf{$\#$Landmark} & \textbf{Distance} \\ \hline
1 & 4.002  \\ \hline
2 & 4.4831 \\ \hline
3 & 4.2959 \\ \hline
4 & 4.3865 \\ \hline
5 & 4.2925 \\ \hline
6 & 5.3631 \\ \hline
7 & 4.636 \\ \hline
8 & 4.9363 \\ \hline
\end{tabular}
\caption{The average error distance per landmark}
\label{tabledistance}
\end{table}

\begin{table}[htbp]
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{$\#$Landmark} & \textbf{Error 1} & \textbf{Error 2} & \textbf{Improved($\%$)} \\ \hline
\textbf{1} & \textbf{4.002} & \textbf{2.486} & \textbf{37.88} \\ \hline
2 & 4.4831 & 2.720 & 39.33 \\ \hline
3 & 4.2959  & 2.652 & 38.27 \\ \hline
4 & 4.3865  & 2.771 & 36.83 \\ \hline
5 & 4.2925  & 2.487 & 42.06 \\ \hline
\textbf{6} & \textbf{5.3631}  & \textbf{3.049} & \textbf{43.15} \\ \hline
7 & 4.636  & 2.684 & 42.11 \\ \hline
8 & 4.9363  & 2.871 & 41.84 \\ \hline
\end{tabular}
\caption{The average error distance per landmark.}
\label{tab2}
\end{center}
\end{table}