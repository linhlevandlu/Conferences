% File example.tex
% Contact: simonnet@ecole.ensicaen.fr
%
% version 1.0 (July 24, 2009)
% version 1.1 (September 3, 2009)
% add using of the optional command: \secondauthoraddr

\documentclass[10pt]{article}

\input{includes/icdp2009template.tex}

%other package

% vectorial font
%\usepackage{lmodern}

\usepackage{graphicx}
\usepackage{times}

\usepackage{textcomp}
\usepackage[justification=centering]{caption}
\usepackage{subcaption}
\usepackage{enumitem}
\usepackage[caption=false]{subfig}
\usepackage{enumitem}
\usepackage{multirow}

\begin{document}
\noindent

% This should produce references in the order they appear
\bibliographystyle{ieeetr}

\title{Towards landmarks prediction with Deep Network}
%\title{Towards landmarks prediction using Convolutional Neural Networks}

\authorname{Van Linh Le$^{1,3}$, Marie Beurton-Aimar$^{1}$, Akka Zemmari$^1$, Nicolas Parisey$^2$}
\authoraddr{$^1$LaBRI - CNRS 5800 Bordeaux University, France, van-linh.le/beurton/zemmari@labri.fr}

%optional
\secondauthoraddr{$^2$IGEPP - INRA 1349, France, nparisey@rennes.inra.fr }
\thirdauthoraddr{$^3$ITDLU - Dalat University, Vietnam, linhlv@dlu.edu.vn}


\maketitle

\keywords
Landmarks, convolutional neural networks, fine-tuning, recogntion, morphometry analysis.

\abstract
Morphometry landmarks are used in many biological
applications. Mostly, the landmarks are defined manually or
semi-automatically by applying image processing techniques. In recent
years, Deep Learning is known as a solution to achieve image analysis tasks such as
classification, recognition, or face detection. In this context, we
present a Convolutional Neural Network (CNN) model to predict landmarks on 2D
anatomical images, specifically beetle's images. The dataset includes the images of 
collecting from $293$ beetles. For each beetle, $5$ images are available corresponding to \textit{head, pronotum} and \textit{body} parts. For each part, a set of manual landmarks has been positioned by an entomologist. In this work, we have focused on prediction of pronotum landmarks. The proposed CNN model is designed from an \textit{elementary block} of three layers: convolution, pooling, and dropout. The network is trained in two different ways: from scratch or after a step of fine-tuning. The fine-tuning parameters are obtained by training on all parts of beetles before to be applied to the pronotum. The quality of predicted landmarks is evaluated by calculating the distance in pixels between the
coordinates of the predicted and manual landmarks which are considered as the ground truth. The obtained results by applying fine-tuning steps are considered to be statistically good enough to replace the manual ones for the different morphometry analysis.

\section{Introduction}
Morphometrics landmarks (or point of interest) are important features
in many biological investigations. They are used to analyze
the shape of biological organs or organisms. These shape analyses are mainly
based on metrics extractions from landmarks coordinates. Depending on the anatomical part, the number of landmarks may vary. As well as their position can be stayed on the edges or inside the anatomical part. For example, the landmarks on Drosophila wings \cite{drosophilaWings} are inside the wings, near veins, but the landmarks on human ear \cite{cintas2016automatic} can be located at
the ear edge or inside the pinna. Currently, the landmarks are set manually by
the entomologist, one can note that this work is time-consuming and difficult to
reproduce when users change. Therefore, a method that proposes automatically the
coordinates of landmarks could be a concern.

In image processing, segmentation is most often the first and the most
important step. This task remains a bootleneck to compute features of
an image. In some cases, the object of interest is easy to extract and
can be analyzed with the help of a lot of very well-known image
analysis procedures. In a previous study \cite{le2017maelab}, we have analyzed two parts beetle mandibles. These parts are pretty easy to segment (at least with an enough good quality for our needs). In this work, we have applied a set of algorithms based on the Hough Transform procedure \cite{palaniswamy2010automatic}. SIFT
\cite{lowe2004distinctive} and SURF \cite{bay2006surf} algorithms may have also been suitable to work on this topic. But, on pronotum, the question of how to properly segment the object of interest  may have consumed a lot of time to develop or to adapt proper methods. This is why we have turned to way of analyzing images without the need for a segmentation step. The application has been again on beetles images
but on \textit{pronotum, head, and body} parts. As the beetles have not been dissected, their anatomical parts have not been set apart. So image segmentation of each part, as they are still attached to the whole specimen, is problematic and has been given up. Coordinates of manual landmarks for each part
have been provided and are considered as the ground
truth to evaluate the predicted ones by our methods. Fig.\ref{figpronotum} shows
the $8$ landmarks that we are looking for.


To achieve the landmarks prediction, a CNN model \cite{lecun2010convolutional} has been designed using Lasagne
library \cite{lasagne}. From a first model version, the network has been
trained from scratch on the dataset of pronotum images. In a second
step, the training has been modified to include a fine-tuning
\cite{yosinski2014transferable} stage.



\begin{figure}[htbp]
\centering
	\centerline{\includegraphics[scale=0.8]{images/pronotum}}
	\caption{\small{An example of pronotum images and its manual landmarks}}
	\label{figpronotum}
\end{figure}

In the next section, we present related works about automatic
estimation landmarks on 2D images. In section $3$, we present the
architecture of the network and the procedure to enlarge the dataset. In section $4$ we compare the results obtained with the first
model and these ones after fine-tuning. 


\section{Related works}
Deep Learning models are coming from machine learning theory. They
have been introduced in the middle of previous century for artificial
intelligence applications but they encounter several problems to
take real-world cases. More recently, the improvement of computing capacities, both in
memory size and computing time with GPU programming has opened new perspective  
for Deep Learning. Many deep learning architectures have been proposed
to solve the problems of classification \cite{krizhevsky2012imagenet,
  ciregan2012multi}, image recognition \cite{szegedy2015going,
  farabet2013learning, li2015convolutional}, speech recognition
\cite{mikolov2011strategies, hinton2012deep} and language translation
\cite{jean2014using, sutskever2014sequence}. To implement the algorithms, many frameworks have been built such as Caffe
\cite{jia2014caffe}, Theano \cite{2016arXiv160502688short}, Tensorflow
\cite{tensorflow2015},\ldots. These frameworks help
the users to design their application by re-using already proposed network
architectures. In image analysis domain,
Deep Learning, specifically with CNN, can be used to predict the key points in
an image. Yi Sun et al. \cite{sun2013deep} have proposed a cascaded
convolutional network to predict the key points on the human
face. Zhang et al. \cite{zhang2014facial} optimize facial landmarks
detection with a set of related tasks such as head pose estimation,
age estimation, \ldots Cintas et al. \cite{cintas2016automatic} have
introduced a network to predict the landmarks on human ear images to
characterize ear shape for biometric analysis.


In geometric morphometry, landmarks (or points of interest) are important features to describe a shape. Depending on the difficulty to segment the objects inside the images, setting automatic landmarks can rely on different methods. When segmentation can be applied, Lowe et
al. \cite{lowe2004distinctive} have proposed a method to identify the
key points in the 2D image. From the detected key points, the method
is able to match two images. Palaniswamy et
al. \cite{palaniswamy2010automatic} have applied probabilistic Hough
Transform to automatically estimate the landmarks in images of
Drosophila wings. In a previous study \cite{le2017maelab}, we have extended
Palaniswamy's method to detect landmarks automatically on beetles
mandibles with good results. Unfortunately, when the segmentation is not precise, we have observed that the results are getting worse. This is why we have turned our work on Deep Learning algorithms in order to find a suitable solution to predict the landmarks without any segmentation step.


\section{Network model}
Deep Learning is a learning method with multiple levels of
representation of connected layers. 
Data representation is transformed from a lower level to a
higher one with many complex functions that can be learned via
backpropagation. 
In this section, we present the initial version of the CNN model that we have used
to begin the landmarks prediction. 

\subsection{Network architecture}
\label{secmodel}
The first step to work with CNN is to define the network
architecture. After several tests, we have chosen to work with a model provided in Lasagne framework \cite{lasagne} coming from
Theano \cite{2016arXiv160502688short}. We will first present the
original model and then, we will describe how we have improved it by definition of an
\textit{elementary block} that we compose in the final model.

Like the networks have been proposed by Cintas et al. \cite{cintas2016automatic}, Li et al. \cite{li2015convolutional}, and  LeCun et al. \cite{lecun2010convolutional}, the proposed network consists of common layers
with different learnable parameters. It receives an input image with
the size  $1 \times 256 \times 192$ to train, to validate, and to
test. The network consists of three repeated structures of a convolutional layer
followed by a pooling layer (keeping the maximum value). The depth of the convolutional layers increases with the different sizes of the filter kernels (i.e, $2 \times 2, 3 \times 3$). All the kernels of pooling layers have the same size (i.e, $2 \times 2$). In the end, three full connected layers are added to the
network. The output of the last full-connected
layer corresponds to the $16$ values which are the 2-coordinates $(x,y)$ of the
$8$ landmarks to predict.


Experiments with this original model show that this architecture is still
not good enough to predict the landmark positions precisely. For
instance, overfitting appears during training and validation
steps. Srivastava et al. \cite{srivastava2014dropout} suggest to use
dropout sequence to correct overfitting artifacts. Dropout step randomly drops units from the
neural network during training and so includes some variations between
the different runs. We have updated the model architecture in that
way. An \textit{elementary block} is defined as a sequence of
convolution (\textit{$C_i$}), pooling (\textit{$P_i$}) and dropout(\textit{$D_i$}) layers that can be repeated several
times before to achieve the computation with the full-connected
layers. For our purpose, we have assembled $3$ \textit{elementary
  blocks} in our model (see Fig.\ref{cnnnetwork2}). The parameters for
each layer are as below, the list of values follows the order of
\textit{elementary blocks}:

\begin{itemize}[nosep,label=\footnotesize$\bullet$]

\item CONV layers:
		\begin{itemize}[nosep]
			\item Number of filters: $32, 64,$ and $128$,
			\item Kernel filters size: $(3 \times 3), (2 \times 2),$ and $(2 \times 2)$,
			\item Stride values: $1, 1, 1$,
			\item No padding is used for CONV layers.
		\end{itemize}			
	\item POOL layers:
		\begin{itemize}[nosep]
			\item Kernel filters size: $(2 \times 2), (2 \times 2),$ and $(2 \times 2)$,
			\item Stride values: $2, 2, 2$.
			\item No padding is used for POOL layers.
		\end{itemize}
	\item DROP layers: 
		\begin{itemize}[nosep]
			\item Probabilities: $0.1, 0.2, $ and $0.3$.
		\end{itemize}
	\end{itemize}
In the last full-connected layers (FC), the parameters are: FC1 output:
$1000$, FC2 output: $1000$, FC3  output: $16$. As usual, a dropout layer is
inserted between FC1 and FC2 with a probability equal to $0.5$.


\begin{figure*}[!t]
\centering
\includegraphics[scale=0.32]{images/arch_model}
\caption{{\small{Network architecture using $3$ \textit{elementary blocks}.
  Convolution
  layer in red, pooling in yellow and dropout in green color.}}} 
\label{cnnnetwork2}
\end{figure*}

During training, the values of learnable parameters have been updated
to increase the accuracy of the network by applying gradient descent
in backward phase. Therefore, the network is designed with a small
sharing learning rate and momentum. Their values are updated over
training time to fit with the number of epochs\footnote{An epoch is a
  single pass through the full training set.}. The network is designed
to finish the training in $5,000$ epochs. The learning rate is
initialized at $0.03$ and stopped at $0.00001$, while the momentum is
updated from $0.9$ to $0.9999$.


The implementation of the network has been done
on Lasagne framework \cite{lasagne} which allows computing on GPU. The
network has been trained on NVIDIA TITAN X cards.


\subsection{Data augementation}
\label{sec_data}
The dataset includes $293$ images of beetles (for each anatomical part). All
images are taken with the same camera in the same condition with a
resolution of $3264 \times 2448$. Each image has the manual
landmarks setting by biologists, i.e, pronotum has $8$ manual landmarks. The
experiments have been designed with a testing set which includes $33$ images randomly chosen and the remained $260$ images are used to train and to validate the model.
The images in training and validation sets will be chosen randomly followed
the ratio during setup the network. For performance considerations, in most of CNNs
\cite{lecun2010convolutional, sun2013deep,  krizhevsky2012imagenet,
  cintas2016automatic}, the size of the input is limited to $256
\times 256$ pixels, thus we did down-sampling our images to a new
resolution $256 \times 192$ (to respect the ratio between $x$ and
$y$), of course the coordinates of manual landmarks have been also
scaled to fit with the new resolution.


One of the main characteristics of CNN is that it must use a huge number of data 
and one can consider that only several hundreds of images is
not enough to feed a CNN. Moreover, working with small dataset can push
us again to the popular problem of \textit{overfitting}. A way to enlarge the dataset size has to be considered. In image processing, we usually apply
transform procedures (translation, rotation) to generate a new image. 
Unluckily the methods to compute features through a CNN
most often are translation and rotation independent. Another way to
enlarge the dataset has to be imagined.


A first procedure has been applied to change the value of each color 
channel in the original image. According to that, a constant is
added to one of the RGB channels each time it is used for training.
Each constant is sampled in a uniform distribution $\in [1,N]$ to obtain
a new value caped at $255$. For example, we can add a constant $c = 10$ to the 
red channel of all images in order to generate new images. 
This operation can be done for the three color channels.

The second procedure separates the channels of RGB into
three gray-scale images. As the network works on single channel images
we are able to  generate six versions of the same image, the total number of
images used to train and to validate is $260 \times 7 = 1820$ images
(six versions and original image). This has been an efficient way to
proceed to the dataset expansion. 


\subsection{First results}
\label{sectrain1}
The set of images that have been used for both training and validation
has been built randomly from the original dataset with a ratio of
$60\%$ for training and $40\%$ for  validation. The training step
takes into account a pair of information \textit{(image, manual
  landmark coordinates)}. At the test phase, images without landmarks
are given to the previously trained network to produce output
coordinates of the predicted landmarks. To obtain a fast convergence
during the computing of CNN, it is useful to normalize the pixel
color value between $[0; 1]$ range and the coordinate values have also been normalized  \cite{lecun2012efficient}.

In order to test predictions for all pronotum images (instead of only $33$ images),
we have applied \textit{cross-validation} to choose the test images. For each time,
we have chosen a different fold of $33$ images as testing images, called \textit{round};
the remaining images have been used as training and validation images. Following that, the network will be trained with many
different training datasets and the output model will be used to
predict the landmarks on the images in the corresponding test
set. After $9$ rounds all images have been
tested. Table.\ref{tbltrainingloss} resumes the training losses for
the $9$ rounds.

\begin{table}[h!]
	\centering
	%\small{
	\begin{tabular}{l l l}
	Round & Training loss & Validation loss \\ \hline
	1 & 0.00018 & 0.00019  \\ \hline
	2 & 0.00019 & 0.00021 \\ \hline
	3 & 0.00019 & 0.00026 \\ \hline
	4 & 0.00021 & 0.00029 \\ \hline
	5 & 0.00021 & 0.00029 \\ \hline
	6 & 0.00019 & 0.00018 \\ \hline
	7 & 0.00018 & 0.00018 \\ \hline
	8 & 0.00018 & 0.00021 \\ \hline
	9 & 0.00020 & 0.00027 \\ \hline
	\end{tabular}
	\caption{\small{The losses during training the model on pronotum images dataset}}
	\label{tbltrainingloss}
	%}
\end{table}

The main goal of the computing is to predict the position of landmarks so
the distance (in pixels) between the manual ones (the ground truth)
and the predicted ones has to be now considered. A correlation test
gives us a good correlation between the position of a manual landmark and
its corresponding predicted one. But we have considered that this
measure is not good enough to provide a useful result to biologists. We
have preferred to evaluate the distance in pixels between the ground
truth and the prediction. Table.\ref{tabledistance} shows the
average distance between manual and predicted landmarks for all
images, landmark per landmark. With images of $256 \times 192$ size, we
can consider that an error of $1\%$ corresponds to $2$ pixels that
could be an acceptable error. Unhappily, our results exhibit
average distance of $4$ pixels in the best case, landmark $1$ and more
than $5$ pixels, landmark $6$. Other error distances are more than $2\%$
pixels.


\begin{table}[h!]
	\begin{subtable}{.22\linewidth}
	\centering
	\begin{tabular}{|c|c| }
	\hline
	\textbf{$\#$Landmark} & \textbf{Distance} \\ \hline
	1 & 4.002  \\ \hline
	2 & 4.4831 \\ \hline
	3 & 4.2959 \\ \hline
	4 & 4.3865 \\ \hline
	
	\end{tabular}
	\end{subtable}%
	\hspace{2.5cm}
	\begin{subtable}{.2\linewidth}
	\centering
	\begin{tabular}{|c|c| }
	\hline
	\textbf{$\#$Landmark} & \textbf{Distance} \\ \hline
	5 & 4.2925 \\ \hline
	6 & 5.3631 \\ \hline
	7 & 4.636 \\ \hline
	8 & 4.9363 \\ \hline
	\end{tabular}
	\end{subtable}
	\caption{\small{The average error distance per landmark}}
	\label{tabledistance}
\end{table}

To illustrate this purpose, Fig.\ref{figrsexample} shows the predicted landmarks on two test images. One can note that even some predicted landmarks (Fig.\ref{figsub1})
are closed to the manual ones, in some case (Fig.\ref{figsub2}) the predicted
ones are far from the expect results. The next step has been dedicated
to the improvement of these results.




\begin{figure}[htbp]
    \centering
    \begin{subfigure}[t]{0.25\textwidth}
        \centering
        \includegraphics[scale=1.3]{images/fn_accuracy}
        \caption{\small{Image with well-predicted landmarks}}
        \label{figsub1}
    \end{subfigure}%
    ~ 
    \begin{subfigure}[t]{0.25\textwidth}
        \centering
        \includegraphics[scale=.63]{images/plandmark2}
        \caption{\small{Image with inaccuracy landmarks}}
        \label{figsub2}
    \end{subfigure}
    \caption{\small{The predicted landmarks, in red,  on the images in test set.}}
    \label{figrsexample}
\end{figure}

\section{Fine-tuning to transfer learning}
\label{secimproving}
The proposed network presented in section \ref{sectrain1} has been trained from scratch on the pronotum dataset. As we have discussed, even the statistic test validates the results, when the predicted landmarks are displayed on the image, the results are not enough precise since the average error is still high ($\geq 4$ pixels).


In order to reach more acceptable results for biologists, we have
broadened the model with a step of transfer learning. That is a
method that re-uses model developed for a specific task/dataset
to lead another task (called \textit{target task}) with another dataset. This allows rapid progress and improves the performance of the model on the target task \cite{torrey2009transfer}. The most popular
example has been given with the project ImageNet of Google \cite{imagenet_cvpr09} which has labeled several millions of images. The obtained parameter values which can
be used in another context to classify another dataset, eventually
very different dataset \cite{margeta_mri}. The name of this procedure to re-use parameters
to pre-train a model is currently called \textit{fine-tuning}.

Fine-tuning does not only replace and retrain the model on the new
dataset but also fine-tunes the weights of a trained model by continuing the
backpropagation. Unfortunately, some rapid tests have shown that
re-using ImageNet features has not been relevant for our
application. We have designed a way to reproduce the method with our
own data. It is worth that of course the size of data to pre-train has drastically decreased. For our pre-training step, the network has been trained on the whole dataset including the images of three parts of beetle i.e pronotum, body and head. Then,
the trained model will be used to fine-tune and test on pronotum set.


\subsection{Training data preparation}
As we have mentioned, the training dataset includes a combination of the images from three
sets: pronotum, body, and head (Fig.\ref{figshape3parts}). For each
set, $260$ original images have been chosen randomly for training and
validation. By applying the same procedure in section \ref{sec_data},
the training dataset was enlarged to $5,460$ images ($260 \times 7
\times 3$). However, the number of manual landmarks on each part is
different: \textit{$8$ landmarks on pronotum part, $11$ landmarks on
  body part, and $10$ landmarks on head part} (Fig.\ref{figshape3parts}). The manual landmarks
have a specific meaning for the biologists. So, we cannot insert the
landmarks arbitrary. Instead of to do that, we keep the smallest number of
landmarks among the three parts and remove the supernumerary when it is needed. Specifically, we have removed three landmarks on the body part
($1^{st}; 6^{t}h; 9^{th}$) and two landmarks on the head part ($5{th};
6^{th}$) (Fig.\ref{figshape3parts}).

\begin{figure}[htbp]
        \centering
        \includegraphics[scale=0.3]{images/merge}
    \caption{\small{A presentation of head, pronotum and body part with corresponding manual landmarks}} 
    \label{figshape3parts}
\end{figure}

\subsection{Using fine-tuning for pronotum dataset}
At the first step, the network is trained with $5,460$ images following
the same way than explained in Section \ref{secmodel}. After that, 
this trained model is used to fine-tune the pronotum dataset. To compare the result with
the previous one, the trained model has been fine-tuned on pronotum images with different cross-validation (as described in section \ref{sectrain1}). The losses during fine-tuning are shown in
Table.\ref{tblfinetuningloss}. Comparing with the losses when we
trained the model from scratch (Table.\ref{tbltrainingloss}), the
validation losses of this scenario have been significantly improved (around
$40\%$).
\begin{table}[h!]
	\centering
	\begin{tabular}{l l l}
	Round & Training loss & Validation loss \\ \hline
	1 & 0.00019 & 0.00009  \\ \hline
	2 & 0.00018 & 0.00010 \\ \hline
	3 & 0.00018 & 0.00010 \\ \hline
	4 & 0.00019 & 0.00008 \\ \hline
	5 & 0.00019 & 0.00009 \\ \hline
	6 & 0.00018 & 0.00008 \\ \hline
	7 & 0.00019 & 0.00008 \\ \hline
	8 & 0.00018 & 0.00006 \\ \hline
	9 & 0.00018 & 0.00009 \\ \hline
	\end{tabular}
	\caption{\small{The losses during fine-tuning model}}
	\label{tblfinetuningloss}
	
\end{table}

At the end, predicted landmarks are given for the test images. The average error based on the distance between
predicted and corresponding manual landmarks has been also computed. The results
are shown in Table.\ref{tab2}. The \textbf{Average} \textit{from scratch} column reminds
the average distance obtained previously. The \textbf{Average} \textit{with fine-tuning}
column presents the new average distance
after fine-tuning the pronotum from the trained model. Besides, the standard errors of both cases have been presented (SD columns). It is
clearly shown that the result of predicted landmarks with the help of
fine-tuning is more precise than the first way to do it.

\begin{table}[htbp]
\centering
\begin{tabular}{ | c | c | c | c | c | }
\hline
	\multicolumn{1}{|c|}{\multirow{2}{*}{Landmark}} & \multicolumn{2}{c|}{From scratch} &  \multicolumn{2}{c|}{With fine-tuning}  \\ \cline{2-5}
	 & Average & SD & Average & SD \  \\ \hline
	\textbf{LM1} & \textbf{4.002} & \textbf{2.5732} & \textbf{2.486} & \textbf{1.5448} \\ \hline
	LM2 & 4.4831 & 2.7583 & 2.7198 & 1.7822 \\ \hline
	LM3 & 4.2959 & 2.7067 & 2.6523 & 1.8386 \\ \hline
	LM4 & 4.3865 & 3.0563 & 2.7709 & 1.9483 \\ \hline
	LM5 & 4.2925 & 2.9086 & 2.4872 & 1.6235 \\ \hline
	\textbf{LM6} & \textbf{5.3631} & \textbf{3.4234} & \textbf{3.0492} & \textbf{1.991} \\ \hline
	LM7 & 4.636 & 2.8426 & 2.6836 & 1.7781 \\ \hline
	LM8 & 4.9363 & 3.0801 & 2.8709 & 1.9662 \\ \hline
\end{tabular}
\caption{\small{A comparing between the average error distances, the standard deviation values per landmark of two steps.}}
\label{tab2}
\end{table}

To illustrate the final results, we display the distribution of the
distance of both the best and the worst results (resp. landmark $1$
and $6$). The Fig.\ref{figrsexample2} shows in (a) and (b) diagrams
 how much the average distances (blue lines) and standard errors (red lines) have
 been improved for the landmark $1$, the (c) and (d) diagrams for the
 landmark $6$.

\begin{figure}[htbp]
   
    \begin{subfigure}[t]{0.25\textwidth}
        \centering
        \includegraphics[scale=.33]{images/lm1_cnn_2}
        \caption{\small{Landmark 1 - CNN}}
        \label{figsub11}
    \end{subfigure}%
    ~ 
    \begin{subfigure}[t]{0.25\textwidth}
        \centering
        \includegraphics[scale=.32]{images/lm1_finetuning_2}
        \caption{\small{Landmark 1 - fine-tuning}}
        \label{figsub22}
    \end{subfigure}~\\
    \begin{subfigure}[t]{0.25\textwidth}
        \centering
        \includegraphics[scale=.33]{images/lm6_cnn_2}
        \caption{\small{Landmark 6 - CNN}}
        \label{figsub111}
    \end{subfigure}%
    ~ 
    \begin{subfigure}[t]{0.25\textwidth}
        \centering
        \includegraphics[scale=.32]{images/lm6_finetuning_2}
        \caption{\small{Landmark 6 - fine-tuning}}
        \label{figsub222}
    \end{subfigure}
    \caption{\small{The distribution of distance error on $1^{st}$ and $6^{th}$ landmarks of all images in two testing steps (CNN and fine-tuning). The blue and red lines present the average distances and standard deviation values, respectively.}}
    \label{figrsexample2}
\end{figure}

As a result of working, the program outputs the
predicted landmarks of the images as TPS files. With the outputs are TPS files,
the user can use MAELab framework\footnote{MAELab is a free software written in C++. It can be directly and freely
obtained by request at the authors.} to display the
landmarks on the images.
\section{Conclusion}
In this paper, we have proposed a CNN model to predict the landmarks on 2D biological images of beetles. The CNN network has been designed from an elementary block repeated three times. This elementary block consists of a convolutional, a max pooling, and a dropout layer. Finally, the elementary blocks are followed after by the usual full connected layers.

In the first step, the model has been trained from scratch and tested on the dataset of pronotum images. In order to improve the results, the model has been trained on a dataset including the images of all three parts of beetles. Then, the trained model has been used to fine-tune and to test on pronotum images.

The results have been evaluated by comparing the coordinates between predicted and manual landmarks.  These results have shown that using the convolutional network to predict the landmarks on biological images leads to provide satisfying results without need of segmentation step on the object of interest. The best set of estimated landmarks has been obtained after a step of fine-tuning using the whole set of images that we have for the project, i.e. about all beetle parts. The quality of prediction allows using automatic landmarking to replace the manual ones. In future works, we plan to study more deeply how to characterize the learning problem to design the right pre-training set.

\section*{Acknowledgements}
The research has been supported by DevMAP project\footnote{https://www6.rennes.inra.fr/igepp\_eng/Research\-teams/Demecology/Projects/INRA\-SPE\-DevMAP}. We would like to thank our colleague, ALEXIA Marie, who have provided manual landmarks on beetle images.

\bibliography{IEEEabrv,includes/icdp2009}


\end{document}
